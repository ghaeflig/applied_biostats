---
title: 'Report 1: Airline Costs analysis'
output:
  pdf_document:
    fig_caption: yes
    latex_engine: pdflatex
  html_document:
    df_print: paged
geometry: margin=2.5cm
papersize: a4
header-includes:
- \usepackage{fancyhdr}
- \usepackage{caption}
- \pagestyle{fancy}
- \fancyhead[L]{Report 1}
- \fancyhead[R]{Monney, Haefliger, Franca, Ferrisse}
fontsize: 12pt
params:
  figwidth: 5
---
```{r, setup, include=FALSE}
# HELP --> https://bookdown.org/yihui/rmarkdown-cookbook/installation.html
# https://bookdown.org/yihui/rmarkdown-cookbook/latex-output.html
# IMPORTANT: you must run this line in your console: tinytex::install_tinytex()
#check that you have pandoc installed
#rmarkdown::find_pandoc()
```

```{=latex}
\thispagestyle{fancy}
```

```{r, include=FALSE}
library(ggplot2)
library(png)
library(knitr)
library(readr)
```

# Introduction/background

<!-- (brief statement of scientific question:) -->
This project aims to explore the relationships between airline costs and multiple factors enumerated down below. This will be done through regression analysis.
<!-- (all variables defined:) -->
The data set was taken from Jesse W. Proctor et al. [A Regression Analysis of Airline Costs, 21 J. AIR L. & COM. 282 (1954)] and consists of 11 variables extracted from 31 airlines. The dependent variable y is the total operating costs (TOC). Its unit of measurement is cents per revenue ton-mile. In order to model the TOC, the following 10 variables were analyzed: the length of the flight, the speed of the plane, the daily flight time, the metropolitan population served, the revenue tons per aircraft mile, the ton-mile load factor, the available capacity, the total assets, the investments and special funds, and the adjusted assets. 
Two variables were determined by other variables: the available capacity corresponds to the revenue tons per aircraft mile (RTM) divided by the ton-mile load factor (LF), and the adjusted assets are equal to the difference between the total assets (TA) and the investments and special funds (I). Therefore, due to these dependencies, RTM, LF, TA and I were removed for the following analysis. Only six independent variables (FL, SoP, DFT, PS, C, AA) were analyzed to predict the total operating cost (TOC). 

```{r, echo=FALSE}
abb <- c("FL", "SoP", "DFT", "PS", "TOC", "RTM", "LF", "C", "TA", "I", "AA")
space <- c("","","","","","","","","","","")
fullnames <- c("Length of flight (miles)", "Speed of plane (miles/hr)", "Daily flight time (hrs)", "Population served (thousands)", "Total operating costs (cents per revenue ton-mile)", "Revenue tons per aircraft mile", "Ton-mile load factor (proportion)", "Available capacity (tons per mile)", "Total assets ($100,000s)", "Investments and special funds ($100,000s)", "Adjusted assets ($100,000s)")
corresp_table <- matrix(data = c(abb,space,fullnames), nrow = 11, ncol = 3)

kable(corresp_table, align='lcr', caption = "Abbreviations of the variables.")
```






# Exploratory data analysis

```{r, include=FALSE}
data <- read.delim("airline_costs.dat", header=FALSE, sep="")
colnames <- c("airline","FL", "SoP", "DFT", "PS", "TOC", "RTM", "LF", "C", "TA", "I", "AA")
colnames(data) <- colnames
ncol(data)
nrow(data)
for (i in 1:ncol(data)) print(typeof(data[,i]))
data_ <- data[,2:ncol(data)] #jsp s'il faut mettre la premiere col en rownames ?
rownames(data_) <- data[,1]
data <- data_
summary(data)

#data 31 x 12
#type of the columns = character, integer or double
#y = column 5
#column 1 = rownames
#--> matrix X of 31x10 with one y 31x1
```



## Univariate graphical

```{r, fig.width=10, fig.height =4.5, echo=FALSE, fig.cap = "Boxplots of the raw data and the logarithm of the data."}
par(mfrow=c(1,2), mar=(c(4,4.1,4.1,2.1))) 
data_log <- log(data)
#data_log[,7] <- data[,7] #REMOVED !
data_log <- data_log[,-c(6,7,9,10)]
data <- data[,-c(6,7,9,10)]
boxplot(data, las=2, main='Boxplots of the raw data')
boxplot(data_log, las=2, main='Boxplots of the log of the data')
```


We can see in the left boxplot that the raw data is unequally distributed; there are very different magnitude values between each factor. This causes an issue when building a model because some factors, due to the fact that they have bigger values, can hold a non-realistic weight which would lead to a biased regression. In order to get the data in the same order of magnitude, we decided to take the logarithm of each variable. From here onward, each time the paper will cite a factor's name, it will in fact be the logarithm of said factor. <!-- except for the LF variable. This variable already has a low magnitude (mean=0.48) and is therefore within the range of magnitude of the transformed variables. -->
<!--Moreover, due dependencies explained in the introduction, RTM, LF, TA and I are removed for the following analysis. In this manner, six independent variables are used to explained the total operating cost. -->

<br /> 

```{r, echo=FALSE}

summary_table <- data.frame()
fivesum <- fivenum(data_log[,1])
mintable <- round(fivesum[1],2)
firstqtable <- round(fivesum[2],2)
mediantable <- round(fivesum[3],2)
thirdqtable <- round(fivesum[4],2)
maxtable <- round(fivesum[5],2)

for (i in 2:ncol(data_log)){
  fivesum <- fivenum(data_log[,i])
  mintable <- c(mintable,round(fivesum[1],2))
  firstqtable <- c(firstqtable,round(fivesum[2],2))
  mediantable <- c(mediantable,round(fivesum[3],2))
  thirdqtable <- c(thirdqtable,round(fivesum[4],2))
  maxtable <- c(maxtable,round(fivesum[5],2))
}

summary_table <- rbind(summary_table, mintable)
summary_table <- rbind(summary_table, firstqtable)
summary_table <- rbind(summary_table, mediantable)
summary_table <- rbind(summary_table, thirdqtable)
summary_table <- rbind(summary_table, maxtable)
names(summary_table) <- c("FL", "SoP", "DFT", "PS", "TOC", "C", "AA")
#names(summary_table) <- c("Length of flight", "Speed of plane", "Daily flight time", "Population served", "Total operating cost", "Revenue tons per aircraft mile", "Ton-mile load factor", "Available Cacity", "Total assets", "Investments and special funds", "Adjusted assets")
rownames(summary_table) <- c("Min","1st Q","Median","3rd Q","Max")
kable(summary_table[,1:7],align='cccccc', caption = "5-numbers summary of the data: Min, 1st Quartile, Median, 3rd Quartile and Max of the log variables.") 
#kable(summary_table[,6:11],align='ccccccc')
```





## Univariate numerical

```{r, echo=FALSE}
summary_table <- data.frame()
meantable <- round(mean(data_log[,1]),2)
sdtable <- round(sd(data_log[,1]),2)
madtable <- round(mad(data_log[,1]),2)
for (i in 2:ncol(data_log)){
  meantable<- c(meantable,round(mean(data_log[,i]),2))
  sdtable <- c(sdtable,round(sd(data_log[,i]),2))
  madtable <- c(madtable,round(mad(data_log[,i]),2))
}

summary_table <- rbind(summary_table, meantable)
summary_table <- rbind(summary_table, sdtable)
summary_table <- rbind(summary_table, madtable)
names(summary_table) <- c("FL", "SoP", "DFT", "PS", "TOC", "C", "AA")
#names(summary_table) <- c("Length of flight", "Speed of plane", "Daily flight time", "Population served", "Total operating cost", "Revenue tons per aircraft mile", "Ton-mile load factor", "Available Cacity", "Total assets", "Investments and special funds", "Adjusted assets")
rownames(summary_table) <- c("Mean", "SD", "MAD")
kable(summary_table[,1:7],align='cccccc', caption = "Mean, standard deviation (SD) and median absolute deviation (MAD) of the logarithm of the variables variables.") 
#kable(summary_table[,6:11],align='ccccccc')
```

\newpage 

## Bivariate numerical (correlations)


```{r bivnum, fig.width=4, fig.height =4, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Correlation values between each feature."}
#Numerical part

#install.packages("Hmisc")
library(Hmisc)
#install.packages("corrplot")
library(corrplot)
#Numerical
#data_cor <- cor(data)
#rcor <- rcorr(as.matrix(data))
#corrplot(data_cor)

data_cor_log <- cor(data_log)
#rcor <- rcorr(as.matrix(log(data))
corrplot(data_cor_log)
#These plots image the correlation values between each feature. Indeed, log data show higher correlation values. (For numerical part of criteria)

```



This diagram demonstrates the correlation between each feature. The bluecolor  illustrates a positive correlation, whereas the red color shows a negative correlation. Negative correlations are visible between the total operating costs and the six other variables. The TOC seems to decrease when the other factors increase. The correlation between the total operating cost and the population served is lower compared to the rest of the variables. 

<br />  

## Bivariate graphical 

This graphics demonstrates shows the scatter plots between each variable. We can find some clear linear relationships between certain features such as between FL and SoP or FL and AA. This can be concluded because we can observe see that the points almost form a straight line between these two factors. We can observe that between TOC and other factors, there are not any straight line; it means that we will need more than one factor to best predict TOC linearly.

\newpage

```{r, fig.width=5, fig.height =5, echo=FALSE, fig.cap="Bivariate scatter plots for each possible interaction"}
pairs(data_log)
#It seems to have more correlation when taking the log of the data ; more straight lines
```



```{r, include=FALSE}
#cost_formula <-  data$TOC ~ data$FL + data$SoP + data$DFT + data$PS + data$RTM + data$LF + data$C + data$TA + data$I + data$AA
#cost.lm <- lm(cost_formula, data=data)
#summary(cost.lm)

#cost_formula <-  data_log$TOC ~ data_log$FL + data_log$SoP + data_log$DFT + data_log$PS + data_log$RTM + data_log$LF + data_log$C + data_log$TA + data_log$I + data_log$AA
library(MASS)
cost_formula <-  data_log$TOC ~ .
cost.lm_log <- lm(cost_formula, data=data_log)
s <- summary(cost.lm_log)
rsq <- s$r.squared
arsq <- s$adj.r.squared
coeff <- cost.lm_log$coefficients
p_val <-pf(s$fstatistic[1], s$fstatistic[2], s$fstatistic[3], lower.tail = FALSE)
summary(cost.lm_log)

stepAIC(cost.lm_log)

final_formula <- data_log$TOC ~ data_log$C + data_log$DFT
final_lm <- lm(final_formula, data=data_log)
summary(final_lm)

final_s <- summary(final_lm)
final_rsq <- final_s$r.squared
final_arsq <- final_s$adj.r.squared
final_coeff <- final_lm$coefficients
final_p_val <-pf(final_s$fstatistic[1], final_s$fstatistic[2], final_s$fstatistic[3], lower.tail = FALSE)


```

# Model fitting 

According to the summary, the equation of our regression model is the following: 

$TOC = \hat{\beta_0} + \hat{\beta_1} \times FL + \hat{\beta_2} \times SoP + \hat{\beta_3} \times DFT + \hat{\beta_4} \times PS + \hat{\beta_5} \times C + \hat{\beta_{6}} \times AA$

$TOC = `r round(coeff[1],2)` + (`r round(coeff[2],2)` \times FL) + (`r round(coeff[3],2)` \times SoP) + (`r round(coeff[4],2)` \times DFT) + (`r round(coeff[5],2)` \times PS) + (`r round(coeff[6],2)` \times C) + (`r round(coeff[7],2)` \times AA)$

$R^2 = `r round(rsq,2)`$ and adjusted $R^2 = `r round(arsq,2)`$   

<br />  

From the initial hypotheses ($H_0: \beta_0 = ... = \beta_{6} = 0$ and $H_1:$ at least one non-zero parameter), the p-value is equal to $`r p_val` < \alpha = 0.05$. The null hypothesis is therefore rejected and we know that at least one coefficient does play a role in predicting the dependent variable. 
<br /> 
The values of the $R^2$ and adjusted $R^2$ are close to 1, which means that the variables are able to closely predict the dependent variable TOC.
<br /> 

\newpage

The p-values of the different features are the following: 
```{r, echo=FALSE}

features <- c("Intercept", "FL", "SoP", "DFT", "PS", "C", "AA")
coeffs <- c(s$coefficients[,4])
corresp_table <- matrix(c(features, round(coeffs,2)),ncol=7,byrow=TRUE)

kable(corresp_table, caption = "P-values")

```

Two features show significant results. These are the daily fight time (DFT) and the available capacity (C), which have a p-value equal to $`r round(s$coefficients[4,4],4)`$ and $`r round(s$coefficients[6,4],3)`$, respectively. Additionaly, both variables have an high correlation with the total operating cost, as shown in the figure 2. 


We will now perform a step-wise model selection by AIC on the previous linear model. The best model found through this method depends on the two variables daily flight time (DFT) and available capacity (C), which corresponds to what has been found previously with the linear regression. This can be explained since these were the only two variables that were significant. Here is the model:


$TOC = \hat{\beta_0} + \hat{\beta_1} \times DFT + \hat{\beta_2} \times C$

$TOC = `r round(final_coeff[1],2)` + (`r round(final_coeff[2],2)` \times DFT) + (`r round(final_coeff[3],2)` \times C)$

The R-squared values are $R^2 = `r round(final_rsq,2)`$ and adjusted $R^2 = `r round(final_arsq,2)`$

From the initial hypotheses ($H_0: \beta_0 = \beta_{1} = \beta_{2} = 0$ and $H_1:$ at least one non-zero parameter), the p-value is equal to $`r final_p_val` < \alpha = 0.05$. The null hypothesis is therefore rejected and we know that at least one coefficient does play a role in predicting the dependent variable. 

The p-values of the different features for our final model are smaller ; they are even close to 0. It confirms the fact that the final model give better results than the previous one ; it has smaller p-values, a greater adjusted R-squared, a lower AIC.

```{r, echo=FALSE}
#features <- c("Intercept", "DFT", "C")
#coeffs <- c(final_s$coefficients[,4])
#corresp_table <- matrix(c(features, round(coeffs,2)),ncol=3,byrow=TRUE)
#kable(corresp_table, caption = "P-values")
# p intercept = < 2e-16
# p C = 4.88e-07
# p DFT = 0.000558
```


<!-- Plus le cas:  In the summary of the linear regression, we can see that the variable RTM has the lowest p-value (=0.040 < 0.05). RTM seems to be the variable that can predict the TOC the best. If we look at figure 2, we can confirm that RTM is one of the most important variable to predict TOC because it has the most prominent colored point indicating a strong correlation between the two variables. -->


# Model assessment

```{r, echo=FALSE, fig.cap = "Residual analysis."}
#layout(matrix(1:6,ncol=3))
#plot(cost.lm, which = c(1,2,3,4,5,6) )   #bad results !!

#log data
#layout(matrix(1:6,ncol=3))
#plot(cost.lm_log, which = c(1,2,3,4,5,6), cex.caption=0.7, cex.id=0.7)

layout(matrix(1:6,ncol=3))
plot(final_lm, which = c(1,2,3,4,5,6), cex.caption=0.7, cex.id=0.7)
#hist(cost.lm_log$resid, main="Histogram of Residuals", ylab="Residuals") #histogramme des résidues

library(lmtest) 
#dwtest(cost.lm_log) #Test pour la correlation... 
bptest(final_lm) #test homoscedacity


```

<!-- From criteria file: 
Assumptions (from report criteria):
1. errors have mean 0 (residuals vs fitted plot) OK
2. errors are homoscedastic (same variance) (scale location plot) OK
3. errors are uncorrelated (residuals vs fitted plot)
4. errors are normally distributed (QQ plot) OK-->


<!--(explication comment interpréter les graphs)
https://data.library.virginia.edu/diagnostic-plots/ 
https://www.andrew.cmu.edu/user/achoulde/94842/lectures/lecture09/lecture09-94842.html -->

From these different plots, the residuals can be evaluated. Many assumptions must be controlled.
Firstly, the normal QQ plot illustrates the normal distribution. In this case, despite the central airlines' data, the standardized residuals are mostly aligned on the theoretical straight line. This confirms that the errors follow a normal distribution. 

The second assumption is that the errors have a mean equal to zero. This can be observed on the residuals vs. fitted plot. Indeed, the residual values are located around 0 with a small variation. This observation means that no non-linear relationship are present in the fitted model. 
From the same plot, the uncorrelated residues can be assumed because the values appear randomly plotted, the red line is horizontal, and no trend is observable.

Concerning the homoscedastic property, the scale-location plot helps to determine it. Indeed, a horizontal line affirms the homoscedasticity of the residues and determines if non-linearity is present. In this model, despite the last point which causes a descending slope, the points are generally randomly spread and form an almost horizontal line. To confirm the homoscedascity, the Breusch-pagan test can be used. This confirms it by indicating a non-significant p-value: the null hypothesis suggesting homoscedacity cannot be rejected.   

Finally, residuals vs. leverage and Cook's distance plots allow to identify potential outliers. No airline errors have a cook's distance higher than 0.4. That means no outliers are observable in this model. 




<!-- old description:
FOR LOG DATA: 
Normal QQ plot shows that absolute values of standardized residuals reach approximately 3 at most, and that no particular deviation from theoretical normal distribution (data follow the theoretical straight line). 

From Residuals vs. fitted plot, we can observe that residuals are spread around a horizontal line at approx. 0. No non-linear relation are present in the model. 

Scale-location allows to observe homoscedasticity. Until fitted value 5, the points are randomly spread and the line is horizontal which affirms the homoscedasticity. After 5, there is some variances, however in a general view the line is quite horizontal. 

Residuals vs. leverage allows to identify potential influencor outliers. We can observe that NorthEast and Lake Central are out of the cook distance (cook's distance higher than 1 (cook's distance plot)) and have influence on the regression.
-->

<!--# Final estimated model -->


# Conclusions
In conclusion, to get the best linear model, we first remove the factors that were dependent. Then, we took the log of the data and we found the best model by AIC selection. At the end, we verified that the usual statistic assumptions. So, our final model to predict TOC is  $TOC = `r round(final_coeff[1],2)` + (`r round(final_coeff[2],2)` \times DFT) + (`r round(final_coeff[3],2)` \times C)$ with a p-value equal to $`r final_p_val` < \alpha = 0.05$ and an adjusted $R^2 = `r round(final_arsq,2)`$.





