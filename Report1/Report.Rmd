---
title: 'Report 1: Airline Costs analysis'
output:
  pdf_document:
    fig_caption: yes
    latex_engine: pdflatex
  html_document:
    df_print: paged
geometry: margin=2.5cm
papersize: a4
header-includes:
- \usepackage{fancyhdr}
- \usepackage{caption}
- \pagestyle{fancy}
- \fancyhead[L]{Report 1}
- \fancyhead[R]{Monney, Haefliger, Franca, Ferrisse}
fontsize: 12pt
params:
  figwidth: 5
---
```{r, setup, include=FALSE}
# HELP --> https://bookdown.org/yihui/rmarkdown-cookbook/installation.html
# https://bookdown.org/yihui/rmarkdown-cookbook/latex-output.html
# IMPORTANT: you must run this line in your console: tinytex::install_tinytex()
#check that you have pandoc installed
#rmarkdown::find_pandoc()
```

```{=latex}
\thispagestyle{fancy}
```

```{r, include=FALSE}
library(ggplot2)
library(png)
library(knitr)
library(readr)
```

# Introduction/background

<!-- (brief statement of scientific question:) -->
This project aims to explore the relationships between airline costs and multiple factors enumerated down below. This will be done through regression analysis.
<!-- (all variables defined:) -->
The data set consists of 11 variables taken in 31 airlines. The dependent variable y is the total operating costs (TOC). Its unit of measurement is cents per revenue ton-mile. In order to model the TOC, the following 10 variables were analyzed: the length of the flight, the speed of the plane, the daily flight time, the metropolitan population served, the revenue tons per aircraft mile, the ton-mile load factor, the available capacity, the total assets, the investments and special funds, and the adjusted assets. 
Two variables were determined by other variables: the available capacity corresponds to the revenue tons per aircraft mile divided by the ton-mile load factor, and the adjusted assets are equal to the difference between the total assets and the investments and special funds. 

```{r, echo=FALSE}
abb <- c("FL", "SoP", "DFT", "PS", "TOC", "RTM", "LF", "C", "TA", "I", "AA")
space <- c("","","","","","","","","","","")
fullnames <- c("Length of flight (miles)", "Speed of plane (miles/hr)", "Daily flight time (hrs)", "Population served (thousands)", "Total operating costs (cents per revenue ton-mile)", "Revenue tons per aircraft mile", "Ton-mile load factor (proportion)", "Available capacity (tons per mile)", "Total assets ($100,000s)", "Investments and special funds ($100,000s)", "Adjusted assets ($100,000s)")
corresp_table <- matrix(data = c(abb,space,fullnames), nrow = 11, ncol = 3)

kable(corresp_table, align='lcr', caption = "Abreviations of the variables.")
```






# Exploratory data analysis

```{r, include=FALSE}
data <- read.delim("airline_costs.dat", header=FALSE, sep="")
colnames <- c("airline","FL", "SoP", "DFT", "PS", "TOC", "RTM", "LF", "C", "TA", "I", "AA")
colnames(data) <- colnames
ncol(data)
nrow(data)
for (i in 1:ncol(data)) print(typeof(data[,i]))
data_ <- data[,2:ncol(data)] #jsp s'il faut mettre la premiere col en rownames ?
rownames(data_) <- data[,1]
data <- data_
summary(data)

#data 31 x 12
#type of the columns = character, integer or double
#y = column 5
#column 1 = rownames
#--> matrix X of 31x10 with one y 31x1
```



## Univariate graphical

```{r, fig.width=10, fig.height =4.5, echo=FALSE, fig.cap = "Boxplots of the raw data and the logarithm of the data."}
par(mfrow=c(1,2), mar=(c(4,4.1,4.1,2.1))) 
boxplot(data, las=2, main='Boxplots of the raw data')
boxplot(log(data), las=2, main='Boxplots of the log of the data')
```

We can see in the left the boxplot that the raw data is unequally distributed; it does not have the same magnitude of values. This causes an issue when building a model because some factors, due to the fact that they have bigger values, can hold a non-realistic weight and therefore, it leads to a biased regression. In order to get the data in the same order of magnitude, we decided to take the logarithm of each variable, except for the LF variable. This variable already has a low magnitude (mean=0.48) and is therefore within the range of magnitude of the transformed variables.


```{r, include=FALSE}
#Take the log of the data
data_log <- log(data)
data_log[,7] <- data[,7]
```


## Univariate numerical

```{r, echo=FALSE}
summary_table <- data.frame()
meantable <- round(mean(data_log[,1]),2)
sdtable <- round(sd(data_log[,1]),2)
madtable <- round(mad(data_log[,1]),2)
for (i in 2:ncol(data_log)){
  meantable<- c(meantable,round(mean(data_log[,i]),2))
  sdtable <- c(sdtable,round(sd(data_log[,i]),2))
  madtable <- c(madtable,round(mad(data_log[,i]),2))
}

summary_table <- rbind(summary_table, meantable)
summary_table <- rbind(summary_table, sdtable)
summary_table <- rbind(summary_table, madtable)
names(summary_table) <- c("FL", "SoP", "DFT", "PS", "TOC", "RTM", "LF", "C", "TA", "I", "AA")
#names(summary_table) <- c("Length of flight", "Speed of plane", "Daily flight time", "Population served", "Total operating cost", "Revenue tons per aircraft mile", "Ton-mile load factor", "Available Cacity", "Total assets", "Investments and special funds", "Adjusted assets")
rownames(summary_table) <- c("Mean", "SD", "MAD")
kable(summary_table[,1:11],align='cccccc', caption = "Mean, standard deviation and median absolute deviation of the variables.") 
#kable(summary_table[,6:11],align='ccccccc')
```


## Bivariate numerical (correlations)


```{r bivnum, fig.width=3.5, fig.height =3.5, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Correlation values between each feature."}
#Numerical part

#install.packages("Hmisc")
library(Hmisc)
#install.packages("corrplot")
library(corrplot)
#Numerical
#data_cor <- cor(data)
#rcor <- rcorr(as.matrix(data))
#corrplot(data_cor)

data_cor_log <- cor(data_log)
#rcor <- rcorr(as.matrix(log(data))
corrplot(data_cor_log)
#These plots image the correlation values between each feature. Indeed, log data show higher correlation values. (For numerical part of criteria)
```



<!-- ## Bivariate graphical (j'ai enlevé cette partie, je pense qu'elle apporte pas grand chose-->


```{r, fig.width=6, fig.height =6, include=FALSE, fig.cap="Bivariate scatter plots for every relations."}
pairs(data_log)
#It seems to have more correlation when taking the log of the data ; more straight lines
```


# Model fitting 


```{r, include=FALSE}
#cost_formula <-  data$TOC ~ data$FL + data$SoP + data$DFT + data$PS + data$RTM + data$LF + data$C + data$TA + data$I + data$AA
#cost.lm <- lm(cost_formula, data=data)
#summary(cost.lm)

#cost_formula <-  data_log$TOC ~ data_log$FL + data_log$SoP + data_log$DFT + data_log$PS + data_log$RTM + data_log$LF + data_log$C + data_log$TA + data_log$I + data_log$AA
cost_formula <-  data_log$TOC ~ .
cost.lm_log <- lm(cost_formula, data=data_log)
s <- summary(cost.lm_log)
rsq <- s$r.squared
arsq <- s$adj.r.squared
coeff <- cost.lm_log$coefficients
summary(cost.lm_log)
```

According to the summary, the equation of our regression model is the following: 

$TOC = \hat{\beta_0} + \hat{\beta_1} \times FL + \hat{\beta_2} \times SoP + \hat{\beta_3} \times DFT + \hat{\beta_4} \times PS + \hat{\beta_5} \times RTM + \hat{\beta_6} \times LF + \hat{\beta_7} \times C + \hat{\beta_8} \times TA + \hat{\beta_9} \times I + \hat{\beta_{10}} \times AA$

$TOC = `r round(coeff[1],2)` + (`r round(coeff[2],2)` \times FL) + (`r round(coeff[3],2)` \times SoP) + (`r round(coeff[4],2)` \times DFT) + (`r round(coeff[5],2)` \times PS) + (`r round(coeff[6],2)` \times RTM) + (`r round(coeff[7],2)` \times LF) + (`r round(coeff[8],2)` \times C) + (`r round(coeff[9],2)` \times TA) + (`r round(coeff[10],2)` \times I) + (`r round(coeff[11],2)` \times AA)$

$R^2 = `r round(rsq,2)`$ and adjusted $R^2 = `r round(arsq,2)`$   

<br />  

From the initial hypotheses ($H_0: \beta_0 = ... = \beta_{10} = 0$ and $H_1:$ at least one non-zero parameter), the p-value is equal to $1.862*10^{-13} < \alpha = 0.05$. The null hypothesis is therefore rejected and we know that at least one coefficient does play a role in predicting the dependent variable. 
<br /> 
The values of the $R^2$ and adjusted $R^2$ are close to 1, which means that the variables are able to closely predict the dependent variable TOC.
<br /> 
In the summary of the linear regression, we can see that the variable RTM has the lowest p-value (=0.040 < 0.05). RTM seems to be the variable that can predict the TOC the best. If we look at figure 2, we can confirm that RTM is one of the most important variable to predict TOC because it has the most prominent colored point indicating a strong correlation between the two variables. 

# Model assessment

```{r, echo=FALSE, fig.cap = "Residual analysis."}
#layout(matrix(1:6,ncol=3))
#plot(cost.lm, which = c(1,2,3,4,5,6) )   #bad results !!

#log data
layout(matrix(1:6,ncol=3))
plot(cost.lm_log, which = c(1,2,3,4,5,6), cex.caption=0.7, cex.id=0.7)
```

Assumptions (from report criteria):
1. errors have mean 0
2. errors are homoscedastic (same variance)
3. errors are uncorrelated
4. errors are normally distributed

FOR LOG DATA: 
Normal QQ plot shows that absolute values of standardized residuals reach approximately 3 at most, and that no particular deviation from theoretical normal distribution (data follow the theoretical straight line). 

From Residuals vs. fitted plot, we can observe that residuals are spread around a horizontal line at approx. 0. No non-linear relation are present in the model. 

Scale-location allows to observe homoscedasticity. Until fitted value 5, the points are randomly spread and the line is horizontal which affirms the homoscedasticity. After 5, there is some variances, however in a general view the line is quite horizontal. 

Residuals vs. leverage allows to identify potential influencor outliers. We can observe that NorthEast and lake central are out of the cook distance (cook's distance higher than 1 (cook's distance plot)) and have influence on the regression.


https://data.library.virginia.edu/diagnostic-plots/ (explication comment interpréter les graphs)

# Final estimated model


# Conclusions




```{r, include=FALSE}
#est-ce que l'on retirerait pas le outliers central ? 
data_log <- log(data)
data_log[,7] <- data[,7]
data_log <- data_log[-c(6),]

cost_formula <-  data_log$TOC ~ .
cost.lm_log <- lm(cost_formula, data=data_log)
s <- summary(cost.lm_log)
rsq <- s$r.squared
arsq <- s$adj.r.squared
coeff <- cost.lm_log$coefficients
summary(cost.lm_log)

layout(matrix(1:6,ncol=3))
plot(cost.lm_log, which = c(1,2,3,4,5,6) )
```

```{r, include=FALSE}
data_log <- log(data)
data_log[,7] <- data[,7]
data_log <- data_log[,-c(6,9,10)]
#retirer les colonne 6(RTM), 9(TA) et 10(I) 
#car Capacité est calculé de cette manière: RTM/LF
#et AA = TA-I, ils sont donc directement relié. dans le paper ils retirent 
#ces paramètres. On a plus de facteurs significatifs de cette manières. 


cost_formula <-  data_log$TOC ~ .
cost.lm_log <- lm(cost_formula, data=data_log)
s <- summary(cost.lm_log)
rsq <- s$r.squared
arsq <- s$adj.r.squared
coeff <- cost.lm_log$coefficients
summary(cost.lm_log)

layout(matrix(1:6,ncol=3))
plot(cost.lm_log, which = c(1,2,3,4,5,6) )
```